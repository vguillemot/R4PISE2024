[
  {
    "objectID": "pages/References.html",
    "href": "pages/References.html",
    "title": "References on PCA, PLS and MFA",
    "section": "",
    "text": "In this page, you will find useful references on the multivariate methods that we will use together during the workshop. These references provide a thorough foundation for understanding PCA, MFA, and PLS, offering both theoretical insights and practical applications."
  },
  {
    "objectID": "pages/References.html#principal-component-analysis-pca",
    "href": "pages/References.html#principal-component-analysis-pca",
    "title": "References on PCA, PLS and MFA",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPrincipal Component Analysis (PCA) is a statistical technique used to simplify the complexity of high-dimensional data while retaining most of its variation. PCA transforms the original data into a new set of uncorrelated variables called principal components. These components are ordered by the amount of variance they explain in the data. The first principal component explains the most variance, followed by the second, and so on. This method is particularly useful for reducing the dimensionality of the data, visualizing patterns, and identifying underlying structures.\nFor more detailed information on PCA, refer to\n\nPrincipal Component Analysis: The Basics,\nor Dr. Hervé Abdi’s webpage: PCA by Hervé Abdi.."
  },
  {
    "objectID": "pages/References.html#multiple-factor-analysis-mfa",
    "href": "pages/References.html#multiple-factor-analysis-mfa",
    "title": "References on PCA, PLS and MFA",
    "section": "Multiple Factor Analysis (MFA)",
    "text": "Multiple Factor Analysis (MFA)\nMultiple Factor Analysis (MFA) is an extension of PCA designed for analyzing data sets that contain multiple groups of variables. Each group of variables is analyzed separately, and then the results are combined to provide a comprehensive analysis. MFA is particularly useful in situations where different sets of variables describe the same set of observations, such as in sensory analysis, where both sensory descriptors and instrumental measurements might be collected.\nDr. Hervé Abdi’s detailed explanation of MFA can be found here: MFA by Hervé Abdi."
  },
  {
    "objectID": "pages/References.html#partial-least-squares-pls",
    "href": "pages/References.html#partial-least-squares-pls",
    "title": "References on PCA, PLS and MFA",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\nPartial Least Squares (PLS) is a statistical method that finds the fundamental relations between two matrices (predictors and responses). It is used when the predictor variables are many and highly collinear. PLS projects the predictor and response variables to new spaces to find the best linear regression model. This technique is widely used in chemometrics, bioinformatics, and social sciences for predictive modeling and feature extraction.\nFor a comprehensive guide on PLS, you can refer to Dr. Hervé Abdi’s work: PLS by Hervé Abdi."
  },
  {
    "objectID": "pages/References.html#additionnal-references",
    "href": "pages/References.html#additionnal-references",
    "title": "References on PCA, PLS and MFA",
    "section": "Additionnal references",
    "text": "Additionnal references\nFor additional resources on these topics, François Husson’s MOOCs provide practical tutorials and examples: Professor Husson’s MOOC."
  },
  {
    "objectID": "pages/FAQ.html",
    "href": "pages/FAQ.html",
    "title": "Frequent errors",
    "section": "",
    "text": "After trying to load a library, you might get an error message, like this one:\n\n\nR code\n&gt; library(missing_package)\nError in library(missing_package) : \n  there is no package called ‘missing_package’\n\n\nIn most cases, it means one of two things:\n\nMaybe you misspelled its name which happens even to the best of us with libraries like data4PCCAR\nThe library you want to load has not been installed yet.\n\nTo install a library, use your favorite installation method: from the “Packages” tab, with a base command like install.packages, or with a command from packages like pak or remotes.\nBelow we show the command using pak to install data4PCCAR:\n\n\nR code\npak::pak(\"HerveAbdi/data4PCCAR\")"
  },
  {
    "objectID": "pages/FAQ.html#the-library-does-not-load",
    "href": "pages/FAQ.html#the-library-does-not-load",
    "title": "Frequent errors",
    "section": "",
    "text": "After trying to load a library, you might get an error message, like this one:\n\n\nR code\n&gt; library(missing_package)\nError in library(missing_package) : \n  there is no package called ‘missing_package’\n\n\nIn most cases, it means one of two things:\n\nMaybe you misspelled its name which happens even to the best of us with libraries like data4PCCAR\nThe library you want to load has not been installed yet.\n\nTo install a library, use your favorite installation method: from the “Packages” tab, with a base command like install.packages, or with a command from packages like pak or remotes.\nBelow we show the command using pak to install data4PCCAR:\n\n\nR code\npak::pak(\"HerveAbdi/data4PCCAR\")"
  },
  {
    "objectID": "pages/FAQ.html#the-function-does-not-exist",
    "href": "pages/FAQ.html#the-function-does-not-exist",
    "title": "Frequent errors",
    "section": "The function does not exist",
    "text": "The function does not exist\nYou might also get this error when trying to use a function:\n\n\nR code\n&gt; missing_function()\nError in missing_function() : could not find function \"missing_function\"\n\n\nIn most cases, it means one of three (!) things:\n\nMaybe you misspelled its name\nThe function belongs to a library that has not be loaded yet\nThe function belongs to a library that has not been installed yet\n\nSo the first thinkg to do is to check the spelling of the function you want to use, and the second to check that you did not forget to load the library.\nFor example, if this missing_function belongs to missing_package, then run library(missing_package) before trying to use your favorite function."
  },
  {
    "objectID": "pages/C_PLSC.html",
    "href": "pages/C_PLSC.html",
    "title": "PLSC on Wines",
    "section": "",
    "text": "R code\n## Load the course library\nlibrary(R4SPISE2022)\n## Load the data\ndata(\"winesOf3Colors\", package = \"data4PCCAR\")\n## Define colors...\n### of the wines...\nwineColors &lt;- list(\n  oc = cbind(as.character(recode(\n    winesOf3Colors$winesDescriptors$color,\n    red = 'indianred4', \n    white = 'gold',\n    rose = 'lightpink2'))),\n  gc = cbind(c(red = 'indianred4', \n                white = 'gold', rose = 'lightpink2'))\n)\n### ... and of the variables\nvarColors &lt;- list(\n  oc = list(\n    cbind(rep(\"darkorange1\", 4)),\n    cbind(rep(\"olivedrab3\", 8))\n  )\n)"
  },
  {
    "objectID": "pages/C_PLSC.html#two-tables-with-descriptors-and-other-supplementary-information",
    "href": "pages/C_PLSC.html#two-tables-with-descriptors-and-other-supplementary-information",
    "title": "PLSC on Wines",
    "section": "Two tables with descriptors and other supplementary information",
    "text": "Two tables with descriptors and other supplementary information\n\n\nR code\ndescr &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(origin, color, varietal)\nsuppl &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(Price)\nchemi &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(Acidity, Alcohol, Sugar, Tannin)\nsenso &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(fruity, floral, vegetal, \n           spicy, woody, sweet, astringent, \n           hedonic)\n\n\n\nTwo data tables\n\nFirst table: chemical data (chemi) that includes acidity, alcohol, sugar, and tannin.\nSecond table: sensory data (senso) that includes fruity, floral, vegetal, spicy, woody, sweet, astringent, and hedonic.\n\n\n\nDescriptors and supplementary variable\n\nDescriptors: origin, color and varietal.\nSupplementary variables: Price.\n\n\n\nR code\n## Run Partial Least Square Correlation (PLS-C) on the two tables\n\nres.pls &lt;- tepPLS(chemi, senso, DESIGN = descr$color, graphs = FALSE)\n### The default of this function will center (to have means equal 0) and scale (to have the *sums of squares* equal 1) all variables in both data tables. The argument `DESIGN` indicates the groups of the observations and change how the observations are colored in the figures (when `graphs = TRUE`), but it does not change the results of PLS-C.\n\n## Generate the figures\n\nres.pls.plot &lt;- TTAplot(\n    res = res.pls, # Output of tepPLS\n    color.obs = wineColors, # &lt;optional&gt; colors of wines\n    color.tab = varColors, # &lt;optional&gt; colors of the two tables\n    tab1.name = \"Chemical data\", # &lt;optional&gt; Name of Table 1 (for printing)\n    tab2.name = \"Sensory data\", # &lt;optional&gt; Name of Table 2 (for printing)\n    DESIGN = descr$color, # design for the wines\n    tab1 = chemi,  # First data table\n    tab2 = senso)  # Second data table\n\n# In `TTAplot`, if `DESIGN` is specified. The latent variables will be colored according to the groups of the observations with the group means and their 95% bootstrap confidence intervals.\n\n\n\n\nCorrelation between the two tables\nWe can check the data by plotting first the correlation matrix between the two data sets. This correlation matrix is where the dimensions are extracted.\n\n\nR code\nres.pls.plot$results.graphs$heatmap.rxy\n\n\n\n\n\n\n\n\n\n\n\nScree plot\nThe scree plot shows the eigenvalues of each dimension. These eigenvalues give the squared covariance of each pair of latent variables. In other words, the singular values, which are the square root of the eigenvalues, give the covariance of these pairs of latent variables. The sum of the eigenvalues is equal to the sum of the squared covariance between all variables in both tables.\n\n\nR code\nres.pls.plot$results.graphs$scree.eig\n\n\n\n\n\n\n\n\n\n\n\nLatent variables\nHere, we plot the first latent variable of both tables against each other with the observations colored according to their groups. This plot shows how the observations are distributed on the dimension and how the chosen pair of latent variables are related to each other. When plotting the first pair of latent variables, we expect the observations to distribute along the bottom-left-to-top-right diagonal line (which illustrates a perfect association), because PLS-C maximizes the covariance of the latent variables.\nTo examine the stability of these groups, we plot the group means with their 95% bootstrap confidence intervals (or ellipsoids). If the ellipses do not overlap, the groups are reliably different from each other. However, it’s worth noted that the distribution of the observations does not imply how the groups (represented by the group means) are distributed or whether the groups are reliably different from each other.\nNote: The grouping information are independent from PLS-C and are only use to help provide a summary description of the observations.\n\n\nR code\nres.pls.plot$results.graphs$lv.plot\n\n\n\n\n\n\n\n\n\nThe results from Dimension 1 show that the association between the chemical and the sensory data reliably separates the red wines from rose and white wines.\n\n\nContributions\nThese bar plots illustrate the signed contribution of variables from the two data tables. From these figures, we use the direction and the magnitude of these signed contributions to interpret the dimension.\nThe direction of the signed contribution is the direction of the loadings, and it shows how the variables contribute to the dimension. The variables that contribute in a similar way have the same sign, and those that contribute in an opposite way will have different signs.\nThe magnitude of the contributions are computed as squared loadings, and they quantify the amount of variance contributed by each variable. Therefore, contribution is similar to the idea of an effect size. To identify the important variables, we find the variables that contribute more than average (i.e., with a big enough effect size). When the variables are centered and scaled to have their sums of squares equals 1, each variable contributes one unit of variance; therefore, the average contribution is 1/(# of variables of the table).\n\n\nR code\nres.pls.plot$results.graphs$ctrX.plot\n\n\n\n\n\n\n\n\n\nR code\nres.pls.plot$results.graphs$ctrY.plot\n\n\n\n\n\n\n\n\n\nFrom these two bar plots, the first dimension is characterized by (1) the positive association between Alcohol and Tannin from the Chemical data and Woody and Astringent from the Sensory data, and (2) the negative association between these variables and Hedonic from the Sensory data.\nTogether with the latent variable plot, we found that, as compared to the rose and the white wines in the sample, the red wines are less Hedonic and stronger in Alcohol, Tannin, Woody, and Astringent.\n\n\nCircles of correlations\nThe circle of correlations illustrate how the variables are correlated with each other and with the dimensions. From this figure, the length of an arrow indicates how much this variable is explained by the two given dimensions. The cosine between any two arrows gives their correlation. The cosine between a variable and an axis gives the correlation between that variable and the corresponding dimension.\nIn this figure, an angle closer to 0° indicates a correlation close to 1; an angle closer to 180° indicates a correlation close to -1; and an 90° angle indicates 0 correlation. However, it’s worth noted that this implication of correlation might only be true within the given dimensions. When a variable is far away from the circle, it is not fully explained by the dimensions, and other dimensions might be characterized by other pattern of relationship between this and other variables.\n\n\nR code\nres.pls.plot$results.graphs$cirCorX.plot\n\n\n\n\n\n\n\n\n\nR code\nres.pls.plot$results.graphs$cirCorY.plot\n\n\n\n\n\n\n\n\n\nThese circles of correlations show that Alcohol, Tannin, Woody, Astringent, and Hedonic are strongly correlated to Dimension 1 with Henodic inversely correlated with all other variables. These variables are mostly explained by the first dimension and have close-to-zero correlation with the second dimension (which is not included and discussed in the previous sections)."
  },
  {
    "objectID": "pages/C_PLSC.html#inference-plots-and-results",
    "href": "pages/C_PLSC.html#inference-plots-and-results",
    "title": "PLSC on Wines",
    "section": "Inference plots and results",
    "text": "Inference plots and results\nThe inference analysis of PCA (performed by OTAplotInference) includes bootstrap test of the proportion of variance explained, permutation tests of the eigenvalues, the bootstrap tests of the loadings (i.e., the left and the right singular vectors).\n\n\nR code\nres.plot.plsc.inference &lt;- TTAplotInference(\n    res = res.pls,\n    tab1 = chemi, \n    tab2 = senso, \n    DESIGN = descr$color,\n    tab1.name = \"Chemical data\", # &lt;optional&gt; Name of Table 1 (for printing)\n    tab2.name = \"Sensory data\" # &lt;optional&gt; Name of Table 2 (for printing)    \n    )\n\n\n\nBootstrap and permutation tests on the eigenvalues\nThe inference scree plot illustrates the 95% bootstrap confidence intervals of the eigenvalues. If an interval includes 0, the eigenvalue is reliably larger than 0.\n\n\nR code\nres.plot.plsc.inference$results.graphs$scree\n\n\n\n\n\n\n\n\n\nThe bootstrap test identifies two significant dimensions having eigenvalues reliably larger than 0. The permutation test identifies two significant dimensions with eigenvalues significantly larger than 0.\n\n\nBootstrap test on the loadings\nThe bar plot illustrates the bootstrap ratios which equals \\[\\frac{M_{p_{j}boot}}{SD_{p_{j}boot}},\\] where \\(M_{p_{j}boot}\\) is the mean of the bootstrapped sample of the jth loading and \\(SD_{p_{j}boot}\\) is the bootstrapped standard deviation of the factor score. A bootstrap ratio is equivalent to a t-statistics for the column factor score with the \\(\\mathrm{H}_0: g_j = 0\\). The threshold is set to 2 to approximate the critical t-value of 1.96 at \\(\\alpha\\) = .05.\n\n\nR code\nres.plot.plsc.inference$results.graphs$BR.X\n\n\n\n\n\n\n\n\n\nThe bootstrap test identifies Alcohol, Sugar, and Tannin as chemical variables with loadings significantly different from 0.\n\n\nR code\nres.plot.plsc.inference$results.graphs$BR.Y\n\n\n\n\n\n\n\n\n\nThe bootstrap test identifies Floral, Spicy, Woody, Sweet, Astringent and Hedonic as significant sensory variables with loadings different from 0.\nIt’s worth noted that the bootstrap ratio gives different information as contributions. Contributions describe the size of the effect, and the bootstrap ratios describe the reliability of the loadings. Therefore, when we interpret the results, we will combine both to draw a conclusion.\n\nInterpreting loadings with contributions and bootstrap ratios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results from the Chemical data show that Alcohol, Sugar, and Tannin all have loadings stably (or significantly) different from 0. But, only Alcohol and Tannin contribute a significant amount of variance to this dimension. Although the loading of Sugar is stable and is significant different from 0, the effect size is small; in other words, it’s not considered as important.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, for the Sensory data, the results show that Floral, Spicy, Woody, Sweet, Astringent and Hedonic all have loadings stably (or significantly) different from 0. But, only Woody, Astringent, and Hedoniccontribute a significant amount of variance to this dimension. In other words, the loadings of Floral, Spicy, Woody, Sweet, Astringent and Hedonic are stable and reliably different from 0, but the important variables that define this dimension are Woody, Astringent, and Hedonic."
  },
  {
    "objectID": "pages/A_Data.html",
    "href": "pages/A_Data.html",
    "title": "Two Sensory Data Set on Wines of The World",
    "section": "",
    "text": "R code\n# Before loading the data, load the following packages\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\n\n# Load the data with the following commands\ndata(\"winesOf3Colors\", package = \"data4PCCAR\")\n# Define the colors for the plots\nwineColors &lt;- winesOf3Colors$winesDescriptors$color\nwineColors &lt;- as.character(recode(wineColors, red = 'indianred4', white = 'gold', rose = 'lightpink2'))"
  },
  {
    "objectID": "pages/A_Data.html#four-tables",
    "href": "pages/A_Data.html#four-tables",
    "title": "Two Sensory Data Set on Wines of The World",
    "section": "Four tables",
    "text": "Four tables\nThere are four types of variables in the data:\n\nDescriptors: origin, color and varietal;\nSupplementary variables: Price;\nChemical data: Acidity, Alcohol, Sugar, and Tannin;\nSensory data: fruity, floral, vegetal, spicy, woody, sweet, astringent, and hedonic."
  },
  {
    "objectID": "pages/A_Data.html#the-sensory-scores",
    "href": "pages/A_Data.html#the-sensory-scores",
    "title": "Two Sensory Data Set on Wines of The World",
    "section": "The sensory scores",
    "text": "The sensory scores"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R4SPISE2024",
    "section": "",
    "text": "Here, you will find data, programs, and documentation for the SPISE2024 Workshop A multi-dimensional approach using MCA, MFA, and PLS with R (from July 24 to July 25, Ho Chi Minh City)."
  },
  {
    "objectID": "index.html#reproducible-examples-in-r",
    "href": "index.html#reproducible-examples-in-r",
    "title": "R4SPISE2024",
    "section": "Reproducible examples in R",
    "text": "Reproducible examples in R\nStudents who are proficient in R are invited to check our reproducible examples out: all the data and code is available on this companion website.\nAll the necessary code to generate the figures and tables “hidden” in collapsed chunks of codes, like this one below.\nClick on the arrow to see the R code.\n\n\nR code\n# This is a comment!\n# And this, below, is a line of R code.\n# Copy and paste it in your own R console\n# to compute the median of three numbers: \n# -1, 0, and 1\nmedian(-1:1)\n# (the result will and should not surprise you)\n\n\n\nInstall our companion package\nWe made the data, as well as cool functions to plot your results, available in an R package that you can dowload and install with pak.\n\n## If needed, install pak first\ninstall.packages(\"pak\")\n## Install R4SPISE2022\npak::pak(\"HerveAbdi/R4SPISE2022\")\n\nThe ouput that you will get will depend on what is already installed on your computer, but should look like this:\n→ Will install 1 package.\n→ Will download 1 package with unknown size.\n+ R4SPISE2022   0.0.0.9000 👷🏿️🔧 ⬇ (GitHub: --------)\nℹ Getting 1 pkg with unknown size\n✔ Got R4SPISE2022 0.0.0.9000 (source) (7.85 MB)             \n✔ Downloaded 1 package (7.85 MB) in 21.8s                   \nℹ Packaging R4SPISE2022 0.0.0.9000\n✔ Packaged R4SPISE2022 0.0.0.9000 (7.4s)                    \nℹ Building R4SPISE2022 0.0.0.9000 \n✔ Built R4SPISE2022 0.0.0.9000 (2.8s) \n\n\nTest\nTo test the installation, run the following chunk of code that will show the help file of function OTAplot.\n\n\nR code\nlibrary(R4SPISE2022)\n\n\nRegistered S3 method overwritten by 'data4PCCAR':\n  method                  from     \n  print.str_colorsOfMusic PTCA4CATA\n\n\nR code\n?OTAplot"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "I am a post-doctoral research fellow at Centre for Addiction and Mental Health (CAMH). My work focuses on developing advanced multivariate methods to integrate and examine the relationships between structural and functional brain connectivity. Prior to joining CAMH, I received my Ph.D. in Cognition & Neuroscience from the University of Texas at Dallas, where I focused on developing advanced multivariate methods (with sparsification and for novel applications) to analyze neuroscience data (including behavioral data, genetics data, fMRI data, and resting-state fMRI data)."
  },
  {
    "objectID": "about.html#ju-chi-yu",
    "href": "about.html#ju-chi-yu",
    "title": "About us",
    "section": "",
    "text": "I am a post-doctoral research fellow at Centre for Addiction and Mental Health (CAMH). My work focuses on developing advanced multivariate methods to integrate and examine the relationships between structural and functional brain connectivity. Prior to joining CAMH, I received my Ph.D. in Cognition & Neuroscience from the University of Texas at Dallas, where I focused on developing advanced multivariate methods (with sparsification and for novel applications) to analyze neuroscience data (including behavioral data, genetics data, fMRI data, and resting-state fMRI data)."
  },
  {
    "objectID": "about.html#hervé-abdi",
    "href": "about.html#hervé-abdi",
    "title": "About us",
    "section": "Hervé Abdi",
    "text": "Hervé Abdi\n\n\n\n\n\n\n\n\n\nI am a Professor at the University of Texas at Dallas in the School of Behavioral and Brain Sciences. My recent work focuses on face and person perception, odor perception, and computational modeling of these processes. I have developed statistical techniques to analyze large data sets, such as those found in genomics, brain imaging, and sensory evaluation, including principal component analysis and multiple factor analysis."
  },
  {
    "objectID": "about.html#vincent-guillemot",
    "href": "about.html#vincent-guillemot",
    "title": "About us",
    "section": "Vincent Guillemot",
    "text": "Vincent Guillemot\n\n\n\n\n\n\nI am currently a biostatistician in the Hub of Bioinformatics and Biostatistics. Before that, I worked at the Brain and Spine Institute (Paris, France), NeuroSpin (Saclay, France) and in the Ludwig Maximilian University (Munich, Germany). My fields of expertise include biostatistics, multivariate statistics, data visualization, statistical data integration and machine learning. My teaching activities range from the introduction to basic concepts in statistics to theoretical aspects in convex optimization."
  },
  {
    "objectID": "pages/E_PLSR.html",
    "href": "pages/E_PLSR.html",
    "title": "PLSR on Wines",
    "section": "",
    "text": "R code\n## Load the course library\nlibrary(R4SPISE2022)\n## Load the data\ndata(\"winesOf3Colors\", package = \"data4PCCAR\")\n## Define colors...\n### of the wines...\nwineColors &lt;- list(\n  oc = cbind(as.character(recode(\n    winesOf3Colors$winesDescriptors$color,\n    red = 'indianred4', \n    white = 'gold',\n    rose = 'lightpink2'))),\n  gc = cbind(c(red = 'indianred4', \n                white = 'gold', rose = 'lightpink2'))\n)\n### ... and of the variables\nvarColors &lt;- list(\n  oc = list(\n    cbind(rep(\"darkorange1\", 4)),\n    cbind(rep(\"olivedrab3\", 1))\n  )\n)"
  },
  {
    "objectID": "pages/E_PLSR.html#two-tables-with-descriptors-and-other-supplementary-information",
    "href": "pages/E_PLSR.html#two-tables-with-descriptors-and-other-supplementary-information",
    "title": "PLSR on Wines",
    "section": "Two tables with descriptors and other supplementary information",
    "text": "Two tables with descriptors and other supplementary information\n\n\nR code\ndescr &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(origin, color, varietal)\nsuppl &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(Price)\nchemi &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(Acidity, Alcohol, Sugar, Tannin)\nsenso &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(fruity, floral, vegetal, \n           spicy, woody, sweet, astringent)\nchemisenso &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(Acidity, Alcohol, Sugar, Tannin, fruity, floral, vegetal,\n           spicy, woody, sweet, astringent)\nhedo &lt;- winesOf3Colors$winesDescriptors %&gt;% \n    select(hedonic)\n\n\n\nPLS-R: Predicting one variable with a data table\n\nThe predictors (X): chemical data (chemi) that includes acidity, alcohol, sugar, and tannin.\nThe predicted (Y): hedonic (hedo).\n\n\n\nDescriptors and supplementary variable\n\nDescriptors: origin, color and varietal.\nSupplementary variables: Price."
  },
  {
    "objectID": "pages/E_PLSR.html#pls-r",
    "href": "pages/E_PLSR.html#pls-r",
    "title": "PLSR on Wines",
    "section": "PLS-R",
    "text": "PLS-R\nLike in a regression model, PLS considers two data tables:\n\nthe first is the predictor table (\\(\\mathbf{X}\\))\nthe second is the (unidimensionnal) predicted table (\\(\\mathbf{y}\\)).\n\nIt aims at solving the regression model [ = + . ]\n\n\nR code\nres.plsr &lt;- data4PCCAR::PLSR_SVD(chemi, hedo, 3, inference = TRUE, displayJack = TRUE)\n# The function will center (to have means equal 0) and scale (to have the _standard deviation_ equal 1) all variables in both data tables.\n## Generate the figures\n## You might need to load these packages if the error is saying that it couldn't find some functions\n# library(PTCA4CATA) \n# library(data4PCCAR)\nres.plsr.plot &lt;- PLSRplot(\n    res = res.plsr, # Output of tepPLS\n    displayJack = TRUE,\n    color.obs = wineColors, # &lt;optional&gt; colors of wines\n    color.tab = varColors, # &lt;optional&gt; colors of the two tables\n    tab1.name = \"Chemical data\", # &lt;optional&gt; Name of Table 1 (for printing)\n    tab2.name = \"Hedonic\", # &lt;optional&gt; Name of Table 2 (for printing)\n    DESIGN = descr$color, # design for the wines\n    tab1 = chemi,  # First data table\n    tab2 = hedo)  # Second data table\n## In this `PLSRplot` function, if `DESIGN` is specified. The latent variables will be colored according to the groups of the observations with the group means and their 95% bootstrap confidence intervals.\n\n\n\nCorrelation between the two tables\nWe can check the data by plotting first the correlation matrix between the two data sets. This correlation matrix is where the dimensions are extracted.\n\n\nR code\nres.plsr.plot$results.graphs$heatmap.rxy\n\n\n\n\n\n\n\n\n\n\n\nScree plot\nThe scree plot shows the eigenvalues of each dimension. These eigenvalues give the squared coefficient of regression (\\(\\beta^2\\) = NA) of each pair of latent variables. In other words, the singular values, which are the square root of the eigenvalues, give the coefficient of regression (\\(\\beta\\) = NA) of these pairs of latent variables. The sum of the eigenvalues is equal to the sum of the squared correlation between all variables in both tables.\n\n\nR code\nres.plsr.plot$results.graphs$scree.eig.R2X\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$scree.eig.R2Y\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$scree.sv.R2X\n#&gt; NULL\nres.plsr.plot$results.graphs$scree.sv.R2Y\n#&gt; NULL\nres.plsr.plot$results.graphs$Y.RESS.plot\n\n\n\n\n\n\n\n\n\n\n\nThe prediction (fixed effect)\n\n\nR code\nres.plsr.plot$results.graphs$YYhat.plot\n\n\n\n\n\n\n\n\n\n\n\nLatent variables\nHere, we plot the first latent variable of both tables against each other with the observations colored according to their groups. This plot shows how the observations are distributed on the dimension and how the chosen pair of latent variables are related to each other. When plotting the first pair of latent variables, we expect the observations to distribute along the bottom-left-to-top-right diagonal line (which illustrates a perfect association), because RA maximizes the coefficient of regression of the latent variables.\nTo examine the stability of these groups, we plot the group means with their 95% bootstrap confidence intervals (or ellipsoids). If the ellipses do not overlap, the groups are reliably different from each other. However, it’s worth noted that the distribution of the observations does not imply how the groups (represented by the group means) are distributed or whether the groups are reliably different from each other.\nNote: The grouping information are independent from RA and are only use to help provide a summary description of the observations.\n\n\nR code\nres.plsr.plot$results.graphs$lv.plot\n\n\n\n\n\n\n\n\n\nThe results from Dimension 1 show that the chemical data can predict the performance of the sensory data, and such prediction separates the red wines from the rosé and the white wines, but not the rosé and the white.\n\n\nContributions\nThese bar plots illustrate the signed contribution of variables from the two data tables. From these figures, we use the direction and the magnitude of these signed contributions to interpret the dimension.\nThe direction of the signed contribution is the direction of the loadings, and it shows how the variables contribute to the dimension. The variables that contribute in a similar way have the same sign, and those that contribute in an opposite way will have different signs.\nThe magnitude of the contributions are computed as squared loadings, and they quantify the amount of variance contributed by each variable. Therefore, contribution is similar to the idea of an effect size. To identify the important variables, we find the variables that contribute more than average (i.e., with a big enough effect size). When the variables are centered and scaled to have their sums of squares equals 1, each variable contributes one unit of variance; therefore, the average contribution is 1/(# of variables of the table).\n\n\nR code\nres.plsr.plot$results.graphs$ctrW1.plot\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$ctrW2.plot\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$LoadingsMap.X\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$ctrC1.plot\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$ctrC2.plot\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$LoadingsMap.Y\n\n\n\n\n\n\n\n\n\nFrom these two bar plots, the first dimension is characterized by (1) the positive association between Alcohol and Tannin from the Chemical data and Woody and Astringent from the Sensory data, and (2) the negative association between these variables and Hedonic from the Sensory data.\nTogether with the latent variable plot, we found that, as compared to the rosé and the white wines in the sample, the red wines are less Hedonic and stronger in Alcohol, Tannin, Woody, and Astringent.\n\n\nCircles of correlations\nThe circle of correlations illustrate how the variables are correlated with each other and with the dimensions. From this figure, the length of an arrow indicates how much this variable is explained by the two given dimensions. The cosine between any two arrows gives their correlation. The cosine between a variable and an axis gives the correlation between that variable and the corresponding dimension.\nIn this figure, an angle closer to 0° indicates a correlation close to 1; an angle closer to 180° indicates a correlation close to -1; and an 90° angle indicates 0 correlation. However, it’s worth noted that this implication of correlation might only be true within the given dimensions. When a variable is far away from the circle, it is not fully explained by the dimensions, and other dimensions might be characterized by other pattern of relationship between this and other variables.\n\n\nR code\nres.plsr.plot$results.graphs$heatmap.rty\n\n\n\n\n\n\n\n\n\nR code\nres.plsr.plot$results.graphs$cirCorY.plot\n\n\n\n\n\n\n\n\n\nThese circles of correlations show that Alcohol, Tannin, Woody, Astringent, and Hedonic are strongly correlated to Dimension 1 with Henodic inversely correlated with all other variables. These variables are mostly explained by the first dimension and have close-to-zero correlation with the second dimension (which is not included and discussed in the previous sections)."
  },
  {
    "objectID": "pages/E_PLSR.html#inference-plots-and-results",
    "href": "pages/E_PLSR.html#inference-plots-and-results",
    "title": "PLSR on Wines",
    "section": "Inference plots and results",
    "text": "Inference plots and results\n\nThe prediction (random effect)\n\n\nR code\nres.plsr.plot$results.graphs$YYjack.plot\n\n\n\n\n\n\n\n\n\n\n\nThe predicted residuals estimated sum of squares (PRESS) of Y (random effect)\n\n\nR code\nres.plsr.plot$results.graphs$Y.PRESS.plot"
  },
  {
    "objectID": "pages/B_PCA.html",
    "href": "pages/B_PCA.html",
    "title": "PCA on Wines",
    "section": "",
    "text": "R code\n## The following code loads the libraries needed for the analysis:\nlibrary(dplyr)\nlibrary(ExPosition)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(InPosition)\nlibrary(R4SPISE2022)\n\n## Ensures reproducibility of bootstrap and permutations\nset.seed(564)\n\n## Load the Wine data\ndata(\"winesOf3Colors\", package = \"data4PCCAR\")\nwines &lt;- winesOf3Colors$winesDescriptors[,4:17]\n\n## Define colors...\n### of the wines...\nwineColors &lt;- list(\n  oc = cbind(as.character(recode(\n    winesOf3Colors$winesDescriptors$color,\n    red = 'indianred4', \n    white = 'gold',\n    rose = 'lightpink2'))),\n  gc = cbind(c(red = 'indianred4', \n                white = 'gold', rose = 'lightpink2'))\n)\n### ... and of the variables\nvarColors &lt;- list(\n  oc = list(\n    cbind(rep(\"darkorange1\", 4)),\n    cbind(rep(\"olivedrab3\", 8))\n  )\n)\n\n## Run the Principal Component Analysis (PCA)\nres.pca &lt;- epPCA(wines, \n                 center = TRUE, # Center the data\n                 scale = TRUE, # Normalize the variables\n                 graphs = FALSE) \n# By default, this function will preprocess the variables in the data set (i.e., the columns of the data table) by 1) centering them (`option center = TRUE` after pre-processing all variables have mean equals to 0) and 2) scaling them (`option scale = TRUE` after pre-processing all variables have a sum of squares equals to \\eqn{N - 1}, \\eqn{N} being the number of rows).\n\n## Generate the plots\nres.plot.pca &lt;- OTAplot(\n    resPCA = res.pca,\n    data = wines, \n    col4I = wineColors$oc\n    )\n## What's in the results:\n# -   `results.stats`: useful statistics after performing PCA, among which one can find the factor scores, loadings, eigenvalues, and contributions;\n# -   `results.graphs`: all the PCA graphs (heatmaps, screeplot, factor map, correlation circles, loading maps etc.)\n# -   `description.graphs`: the titles of these graphs, used as titles in the PPTX file."
  },
  {
    "objectID": "pages/B_PCA.html#inference-plots-and-results",
    "href": "pages/B_PCA.html#inference-plots-and-results",
    "title": "PCA on Wines",
    "section": "Inference plots and results",
    "text": "Inference plots and results\nThe inference analysis of PCA (performed by OTAplotInference) includes bootstrap test of the proportion of variance explained, permutation tests of the eigenvalues, the bootstrap tests of the column factor scores \\(\\mathbf{G}\\) (i.e., the right singular vectors scaled to have the variance of each dimension equals the corresponding singular value; \\(\\mathbf{G} = \\mathbf{Q}\\boldsymbol{\\Delta}\\) with \\(\\mathbf{X = P}\\boldsymbol{\\Delta}\\mathbf{Q}^\\top\\)). These column factor scores are stored as fj in the output of epPCA.\n\n\nR code\nres.plot.pca.inference &lt;- OTAplotInference(\n    resPCA = res.pca,\n    data = wines,\n    col4I = wineColors$oc\n    ) %&gt;% suppressMessages()\n\n\n\nBootstrap and permutation tests on the eigenvalues\nThe inference scree plot illustrates the 95% bootstrap confidence intervals of the percentage of variance explained by each eigenvalue. If an interval includes 0, the component explains reliably larger than 0% of the variance.\n\n\nR code\nres.plot.pca.inference$results.graphs$scree\n\n\n\n\n\n\n\n\n\nThe bootstrap test identifies one significant dimension explains the variance reliably larger than 0%. The permutation test identifies one significant dimension with an eigenvalue significantly larger than 0.\n\n\nBootstrap test on the column factor scores\nThe bar plot illustrates the bootstrap ratios which equals \\[\\frac{M_{g_{j}boot}}{SD_{g_{j}boot}},\\] where \\(M_{g_{j}boot}\\) is the mean of the bootstrapped sample of the jth column factor score and \\(SD_{g_{j}boot}\\) is the bootstrapped standard deviation of the factor score. A bootstrap ratio is equivalent to a t-statistics for the column factor score with the \\(\\mathrm{H}_0: g_j = 0\\). The threshold is set to 2 to approximate the critical t-value of 1.96 at \\(\\alpha\\) = .05.\n\n\nR code\nres.plot.pca.inference$results.graphs$BR1\n\n\n\n\n\n\n\n\n\n\n\nR code\nres.plot.pca.inference$results.graphs$BR2\n\n\n\n\n\n\n\n\n\nThe results show that Astringent is the only significant contributor to Dimension 1, and there is no significant factor scores for Dimension 1 that is reliably different from 0."
  },
  {
    "objectID": "pages/D_MFA.html",
    "href": "pages/D_MFA.html",
    "title": "Multiple Factor Analysis on Wines",
    "section": "",
    "text": "R code\n# You will need the following libraries\n# for this example, if these are not installed\n# install them with `remotes`, `pak`\n\n# de-comment the following lines to\n# install missing packages\n# install.packages(\"pak\")\n# pak::pak('dplyr')\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\nlibrary(knitr)\nlibrary(kableExtra)\n#&gt; \n#&gt; Attaching package: 'kableExtra'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     group_rows\nlibrary(ggplot2)\nlibrary(TExPosition)\n#&gt; Loading required package: prettyGraphs\n#&gt; Loading required package: ExPosition\n#pak::pak('HerveAbdi/data4PCCAR')\nlibrary(data4PCCAR)\n#pak::pak('HerveAbdi/PTCA4CATA')\nlibrary(FactoMineR)\nsuppressMessages(library(factoextra))\n#pak::pak('gastonstat/colortools')\nlibrary(colortools)\nlibrary(coin)\n#&gt; Loading required package: survival\n# suppressMessages(library(XLConnect))\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(FactoMineR)\nlibrary(factoextra)\nWe illustrate MFA with the 6 wines and 3 assessors data-set. In this example, three different assessors (named e1, e2, and e3) evaluated 6 wines using their own descriptors.\nR code\ndata('wines2007', package = \"ExPosition\")\n# your data (a concatenated data tables)\n# Have a look\nwines2007$data\n#&gt;    e1.fruity e1.woody e1.coffee e2.red.fruit e2.roasted e2.vanillin e2.woody\n#&gt; W1         1        6         7            2          5           7        6\n#&gt; W2         5        3         2            4          4           4        2\n#&gt; W3         6        1         1            5          2           1        1\n#&gt; W4         7        1         2            7          2           1        2\n#&gt; W5         2        5         4            3          5           6        5\n#&gt; W6         3        4         4            3          5           4        5\n#&gt;    e3.fruity e3.butter e3.woody\n#&gt; W1         3         6        7\n#&gt; W2         4         4        3\n#&gt; W3         7         1        1\n#&gt; W4         2         2        2\n#&gt; W5         2         6        6\n#&gt; W6         1         7        5\n# a data frame with one row that specifies the table membership of each column of `wines2007$data`\n# run mfa\ngroup4MFA &lt;- summary(as.factor(t(wines2007$table)))\nname.group4MFA &lt;- names(summary(as.factor(t(wines2007$table))))\nresMFA &lt;- FactoMineR::MFA(wines2007$data, # data table\n                          group = group4MFA,\n                          name.group = name.group4MFA,\n                          graph = FALSE)\n# The list `resMFA` contains all the information \n# needed to interpret the results of the analysis."
  },
  {
    "objectID": "pages/D_MFA.html#the-mathbfr_v-coefficient-matrix",
    "href": "pages/D_MFA.html#the-mathbfr_v-coefficient-matrix",
    "title": "Multiple Factor Analysis on Wines",
    "section": "The \\(\\mathbf{R_v}\\) coefficient matrix",
    "text": "The \\(\\mathbf{R_v}\\) coefficient matrix\nA first step is to look at the \\(R_V\\) matrix—A matrix that stores the values of the \\(R_v\\) coefficient between pairs of the original data tables (and also with the whole MFA).\nFor MFA, these coefficients are only descriptive and can be interpreted like a squared coefficient of correlation for matrices: a value close to 1 for a pair of data tables indicates that these two tables are storing similar information, a value close to zero indicates that these tables store independent information.\n\n\nR code\n# a nice heatmap\nresMFA$group$RV %&gt;%\n  as_tibble(rownames = \"rows\") %&gt;%\n  pivot_longer(-1, names_to = \"cols\") %&gt;%\n    mutate(\n      cols = factor(cols, levels = rev(unique(cols)))) %&gt;%\n  ggplot(aes(x = rows, y = cols)) +\n  geom_tile(aes(fill = value)) +  \n  geom_text(aes(label = round(value, 2)), color = \"white\", fontface = \"bold\") +  \n  scale_fill_gradient2(low = \"#BB4444\", mid =  \"#FFFFFF\", high = \"#4477AA\", midpoint = 0, limits = c(-1, 1 + 1e-10)) +\n  coord_equal() + \n  scale_x_discrete(position = \"top\") +\n  labs(x = \"\", y = \"\", fill) +\n  theme_minimal() + \n  # theme(text = element_text())\n  geom_hline(yintercept = 1.5, color = \"white\") + \n  geom_vline(xintercept = 3.5, color = \"white\")\n\n\n\n\n\n\n\n\n\nR code\n\n\n# col &lt;- colorRampPalette(\n#   c(\"#BB4444\", \"#EE9988\", \"#FFFFFF\", \"#77AADD\", \"#4477AA\")\n#   )\n# plot.RV &lt;- corrplot::corrplot(resMFA$group$RV, \n#                            method = \"color\",\n#                            col = col(201), \n#                            addCoef.col = \"black\", \n#                            number.cex = 0.8, \n#                            tl.col = \"black\", \n#                            mar = c(0,0,0,0),\n#                            addgrid.col = \"grey\", \n#                            tl.srt = 90)\n# print(plot.RV)"
  },
  {
    "objectID": "pages/D_MFA.html#weights-alpha-in-the-paper-applied-to-each-data-table",
    "href": "pages/D_MFA.html#weights-alpha-in-the-paper-applied-to-each-data-table",
    "title": "Multiple Factor Analysis on Wines",
    "section": "Weights (alpha in the paper) applied to each data table",
    "text": "Weights (alpha in the paper) applied to each data table\nYou can show the alphas (\\(\\boldsymbol{\\alpha}\\)) with a barplot.\n\n\nR code\n#Eig.tab &lt;- demo.mfa.2007$mexPosition.Data$Compromise$compromise.eigs\nEig.tab &lt;- c(resMFA$separate.analyses$E1$eig[1],\n             resMFA$separate.analyses$E1$eig[2],\n             resMFA$separate.analyses$E1$eig[3])\nAlpha &lt;- 1/sqrt(Eig.tab)\n\ntibble(Assessor = paste0(\"E\", 1:3),\n       Alpha = Alpha) %&gt;%\n  ggplot(aes(Assessor, Alpha, fill = Assessor)) + \n  geom_col(show.legend = FALSE) + \n  theme_bw() + \n  labs(y = \"\", title = \"Weight applied to each table\")"
  },
  {
    "objectID": "pages/D_MFA.html#scree-plot",
    "href": "pages/D_MFA.html#scree-plot",
    "title": "Multiple Factor Analysis on Wines",
    "section": "Scree-plot",
    "text": "Scree-plot\n\n\nR code\n#Eig4scree &lt;- demo.mfa.2007$mexPosition.Data$Table$eigs\nEig4scree &lt;- resMFA$global.pca$eig[,1]\nTau4scree &lt;- resMFA$global.pca$eig[,2]\n# Use factorextra\n# Eigenvalues/variances of dimensions\nfviz_screeplot(resMFA)"
  },
  {
    "objectID": "pages/D_MFA.html#global-factor-scores-of-the-rows",
    "href": "pages/D_MFA.html#global-factor-scores-of-the-rows",
    "title": "Multiple Factor Analysis on Wines",
    "section": "Global factor scores of the rows:",
    "text": "Global factor scores of the rows:\nThis shows how the rows are projected onto the space from the point of all tables\n(You want to plot them like how you plot them in PCA: Component 1 vs. Component 2)\n\n\nR code\nfi &lt;- resMFA$global.pca$ind$coord\nfviz_mfa_ind(resMFA, col.ind = \"cos2\",\n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  repel = TRUE)"
  },
  {
    "objectID": "pages/D_MFA.html#partial-factor-scores-of-the-rows-how-the-rows-are-viewed-from-the-persepctive-of-each-table",
    "href": "pages/D_MFA.html#partial-factor-scores-of-the-rows-how-the-rows-are-viewed-from-the-persepctive-of-each-table",
    "title": "Multiple Factor Analysis on Wines",
    "section": "Partial factor scores of the rows: how the rows are viewed from the persepctive of each table",
    "text": "Partial factor scores of the rows: how the rows are viewed from the persepctive of each table\n(You want to plot them with the global factor scores. Please check out the example of DiSTATIS to see how you plot them.)\n\n\nR code\n# Partial individuals\nfviz_mfa_ind(resMFA, partial = \"all\")"
  },
  {
    "objectID": "pages/D_MFA.html#loadings-of-variables",
    "href": "pages/D_MFA.html#loadings-of-variables",
    "title": "Multiple Factor Analysis on Wines",
    "section": "Loadings of variables",
    "text": "Loadings of variables\n(You interpret them as in PCA.)\n\n\nR code\n# Quantitative variables\nfviz_mfa_var(resMFA, \"quanti.var\", palette = \"jco\", \n             repel = TRUE)"
  },
  {
    "objectID": "pages/D_MFA.html#contributions",
    "href": "pages/D_MFA.html#contributions",
    "title": "Multiple Factor Analysis on Wines",
    "section": "Contributions",
    "text": "Contributions\nLeft as an exercise."
  },
  {
    "objectID": "pages/D_MFA.html#bonus-for-r-users-how-to-run-an-mfa",
    "href": "pages/D_MFA.html#bonus-for-r-users-how-to-run-an-mfa",
    "title": "Multiple Factor Analysis on Wines",
    "section": "Bonus for R users: How to run an MFA",
    "text": "Bonus for R users: How to run an MFA\nFactoMineR is the standard R-package for running MFA. As the name indicates, the function FactoMineR::MFA will run the MFA."
  }
]